---
layout: post
title: "Potential risks of AI"
description: ""
categories:
  - AI Policy
date: "16th August 2023"
published: true
edit: https://github.com/MokeEire/yuyutsu/blob/master/src/routes/posts/second-post/%2Bpage.md
image: https://images.unsplash.com/photo-1529027288157-572df421f425?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1170&q=80
header: https://images.unsplash.com/photo-1529027288157-572df421f425?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1170&q=80
---

As artificial intelligence develops, risks will arise from misuse of the technology, accidents throughout development and deployment, and possibly even rogue AI systems themselves. 
To explore the likelihood and danger of each risk, let’s first define some terms. 
I define misuse as knowingly harmful (human) actions that leverage AI, while accidents are unanticipated or unknowingly harmful actions that might occur in development or deployment of AI systems. 
Risks from rogue, agentic AI systems could be considered accidents in situations where AI developers lose control of the system and it “goes rogue”, or misuse in a situation where cybercriminals intentionally unleash such an AI on the world. 
The distinction may be valuable insofar as it describes risks that may lie further outside human control than either of the other categories.

## Misuse

Misuse of AI has already materialized from advances like generative AI and computer vision. Generative AI reduced the cost of conducting disinformation campaigns and enhanced the targeting with the capability to tailor content to individuals, given the right data. 
Computer vision and facial recognition enable authoritarian governments to conduct more effective surveillance to monitor and control their populations. 
These tools have been used in the past few years by China and Russia to suppress protests against China’s actions to enact draconian legislation in Hong Kong and Russia’s invasion of Ukraine respectively. 
Tools and techniques required for bioterrorism will be accessible to non-experts in the next decade, while tools for cybercrime, such as hacking and exploiting internet-connected devices are currently available and regularly becoming more accessible. 
The increasing accessibility of these tools could increase the likelihood of a deadly virus (biological or otherwise) spreading across the globe. 
The scale of potential harm from misuse of AI will likely depend on advances in cybersecurity, the responsiveness of lawmakers, and whether both can keep pace with AI development. 
While the threat of bioterrorism may be the most explicitly existential risk, the misuse of AI may undermine several important functions in society that have wider reaching consequences in the long term.

## Accidents

Accidents in development and deployment of AI systems is another risk posed by AI systems and may have already caused harm in the form of algorithmic bias in systems that influence hiring decisions, criminal sentencing, and creditworthiness. 
As AI systems are integrated into national defense decision-making, the risk of accidents becomes much larger, particularly in the context of nuclear war. 
To the degree that AI provides a competitive advantage, there will be pressure to prioritize progress over safety and diligence. 
Militaries will face pressure to deploy autonomous weapons prematurely to ensure victory in a conflict, especially if defeat is looking likely. 
Corporations will feel pressure to release AI-powered products before thoroughly testing the system to stay ahead of competitors. 
This arms race dynamic will increase the likelihood of accidents and the scale of these risks will increase as AI systems become more capable. 
With each successive accident, we will have an opportunity to revise our practices and the likelihood of future accidents will depend on our responses.

## Rogue, agentic AI systems

An important component of risks from rogue, agentic AI systems that is needed to distinguish them from misuse and accidents is active intent (though not necessarily malicious) of the AI systems themselves. 
In the near term, these kinds of risks seem least likely and least harmful, but I am very uncertain about these estimates. 
We do not have a good baseline for what kind of harm a rogue AI system could inflict. 
The likelihood of this risk depends heavily on advances in our ability to align AI systems, specify robust reward models, and account for the seemingly irreducible human error when specifying tasks. 
Without advances in these areas, it is difficult to see a long term future for humanity. 
If we do make progress, it seems likely that rogue AIs could still represent an infrequent threat similar to the threat of mass shootings in the present.

## Conclusion

I think misuse of AI is all but guaranteed. Accidents will be likely among competitive dynamics, but may motivate policy to reduce the likelihood of further accidents. 
Rogue AIs seem less likely in the near term, but this risk will increase dramatically if we do not make progress on AI alignment in the near future. 
It is important to consider how these risks are interdependent. 
Misuse could increase the frequency of accidents. 
If misuse is prevalent and accidents increase, the likelihood of losing control of AI systems may increase too.

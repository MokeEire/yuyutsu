---
layout: post
title: "What could go wrong with AI?"
description: "An overview of the ongoing harms, risks, and doomsday placards."
categories:
  - AI Policy
date: 2023-08-16
published: true
image: https://images.unsplash.com/photo-1529027288157-572df421f425?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1170&q=80
header: https://images.unsplash.com/photo-1529027288157-572df421f425?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1170&q=80
---

Artificial intelligence (AI) is an exciting and scary technology.
It has already been used to push forward our knowledge of biology, 
As artificial intelligence develops, [scientists are loudly warning the public of the potentially catastrophic risks](https://arxiv.org/abs/2306.12001) from misuse of the technology, accidents in system development and deployment, and possibly even rogue AI systems themselves. 
To explore the likelihood and danger of each risk, letâ€™s first define some terms. 
**Misuse** is knowingly harmful (human) behaviour that leverages AI to achieve malicious aims, while **accidents** are unanticipated or unknowingly harmful actions that occur in development or deployment of AI systems. 
Risks from **rogue, agentic AI systems** could be considered *accidents* in a situation where AI developers lose control of a system and the system "goes rogue", or *misuse* in a situation where cybercriminals intentionally unleash such an AI on the world. 
Distinguishing rogue systems may be valuable insofar as it describes risks that may lie further outside human control than either of the other categories.

## Misuse

Misuse of AI has already materialized from advances like generative AI and computer vision. 
Generative AI reduced the cost of conducting disinformation campaigns and enhanced the targeting with the capability to tailor content to individuals, given the right data. 
Computer vision and facial recognition enable authoritarian governments to conduct more effective population surveillance. 
China has used facial recognition technologies (FRT) [to profile and track Uyghurs](https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html), an ethnic muslim minority group, while Russia used FRT [to identify and arrest anti-government protesters, in some cases as a preventative measure](https://www.reuters.com/investigates/special-report/ukraine-crisis-russia-detentions/), shortly after Russia's invasion of Ukraine. 

But misuse is not limited to powerful authoritarian governments. 
Cybercriminals are leveraging generative AI and machine learning techniques to [better guess passwords, tailor content for social engineering attacks, and autonomously improve performance of cyberattacks](https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/exploiting-ai-how-cybercriminals-misuse-abuse-ai-and-ml).
Advances in biotechnology suggest that [tools and techniques required for engineering pandemic-level biological weapons will be accessible to non-experts in the next decade](https://www.gcsp.ch/publications/delay-detect-defend-preparing-future-which-thousands-can-release-new-pandemics).
Even in cases where AI systems are being used to improve human health through the development of new drugs to combat disease, the researchers discovered [their algorithm could easily be manipulated to develop biological weapons instead](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/).

The scale of harm from misuse of AI will likely depend on advances in cybersecurity, the responsiveness of lawmakers, and whether each can keep pace with AI development. 
While the threat of bioterrorism may be the most explicitly existential risk, misuse of AI may undermine several important functions in society that have wider reaching consequences in the long term.

## Accidents

Accidents in development and deployment of AI systems is another risk posed by AI systems and may have already caused harm in the form of algorithmic bias in systems that influence hiring decisions, criminal sentencing, and creditworthiness. 
As AI systems are integrated into national defense decision-making, the risk of accidents becomes much larger, particularly in the context of nuclear war. 
To the degree that AI provides a competitive advantage, there will be pressure to prioritize progress over safety and diligence. 
Militaries will face pressure to deploy autonomous weapons prematurely to ensure victory in a conflict, especially if defeat is looking likely. 
Corporations will feel pressure to release AI-powered products before thoroughly testing the system to stay ahead of competitors. 
This arms race dynamic will increase the likelihood of accidents and the scale of these risks will increase as AI systems become more capable. 
With each successive accident, we will have an opportunity to revise our practices and the likelihood of future accidents will depend on our responses.

## Rogue, agentic AI systems

An important component of risks from rogue, agentic AI systems that is needed to distinguish them from misuse and accidents is active intent (though not necessarily malicious) of the AI systems themselves. 
In the near term, these kinds of risks seem least likely and least harmful, but I am very uncertain about these estimates. 
We do not have a good baseline for what kind of harm a rogue AI system could inflict. 
The likelihood of this risk depends heavily on advances in our ability to align AI systems, specify robust reward models, and account for the seemingly irreducible human error when specifying tasks. 
Without advances in these areas, it is difficult to see a long term future for humanity. 
If we do make progress, it seems likely that rogue AIs could still represent an infrequent threat similar to the threat of mass shootings in the present.

## Conclusion

I think misuse of AI is all but guaranteed. Accidents will be likely among competitive dynamics, but may motivate policy to reduce the likelihood of further accidents. 
Rogue AIs seem less likely in the near term, but this risk will increase dramatically if we do not make progress on AI alignment in the near future. 
It is important to consider how these risks are interdependent. 
Misuse could increase the frequency of accidents. 
If misuse is prevalent and accidents increase, the likelihood of losing control of AI systems may increase too.

## References

- https://arxiv.org/abs/2306.12001

- https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html

- https://www.reuters.com/investigates/special-report/ukraine-crisis-russia-detentions/

- https://www.gcsp.ch/publications/delay-detect-defend-preparing-future-which-thousands-can-release-new-pandemics

- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/